{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "os.chdir(\"C:/Users/espen/Documents/SDS/thesis\")\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data containing raw html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df_html = pd.read_parquet(\"C:/Users/espen/Documents/SDS/thesis/data/processed/pyarrow/verdicts_in_UfR_only_articles_with_html\")\n",
    "df_html[\"id_verdict\"] = df_html[\"id\"].apply(lambda x: re.sub(\"\\$[0-9]*\",\"\",x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to convert HTML to relevant text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_html_data(string):\n",
    "    if type(string) == str:\n",
    "        soup = BeautifulSoup(string, features=\"lxml\")\n",
    "        s = soup.find(\"div\", {\"class\":\"maincontent\"})\n",
    "        if s is None: return \"\"\n",
    "        s = s.get_text(separator=' ')\n",
    "        s = re.sub(\"\\n\", \" \",s)\n",
    "        s = re.sub(\" +\",\" \",s)\n",
    "        s = s.split(\" Tidsskrifter Ugeskrift for Retsvæsen\",1)[0]\n",
    "    else:\n",
    "        s = \"\"\n",
    "    return s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cleaning functions above on each verdict page (there can be multiple pages pr verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68317/68317 [12:09<00:00, 93.69it/s] \n"
     ]
    }
   ],
   "source": [
    "df_html[\"content\"] = df_html[\"html_data\"].progress_apply(lambda x: retrieve_html_data(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge (potential) multiple pages into one observation pr verdict and clean df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce \n",
    "def append_html_body_soup(html_1, html_2=None):\n",
    "    \n",
    "    first_page_soup = BeautifulSoup(html_1)\n",
    "    second_page_soup = BeautifulSoup(html_2)\n",
    "\n",
    "    [first_page_soup.body.append(element) for element in second_page_soup.body()]\n",
    "    \n",
    "    return str(first_page_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63915/63915 [18:40<00:00, 57.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "for verdict in tqdm(df_html[\"id_verdict\"].unique()):\n",
    "    if len(pages_df:=df_html.loc[df_html[\"id_verdict\"]==verdict])>1:\n",
    "        df_html.loc[df_html[\"id_verdict\"]==verdict,\"html_concat\"]=reduce(append_html_body_soup, list(pages_df[\"html_data\"]))\n",
    "    else:\n",
    "        df_html.loc[df_html[\"id_verdict\"]==verdict,\"html_concat\"]=pages_df[\"html_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_html.copy()\n",
    "func = lambda x: \" \".join(x)\n",
    "df = df.loc[:,[\"content\",\"id_verdict\"]].groupby(\"id_verdict\").agg(func).rename(columns={\"content\":\"verdict_text\"})\n",
    "df = pd.merge(df,df_html, left_on=\"id_verdict\",right_on=\"id_verdict\")\n",
    "df = df.drop_duplicates(\"id_verdict\")\n",
    "df = df.drop([\"id\",\"karnov_pagenation\",\"id_dir_compliant\",\"next_page\",\"prev_page\", \"url\",\"html_data\", \"content\", \"directory\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df as pyarrow table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table,\"data/processed/pyarrow/UfR_text.parquet\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ed8ad36289d305c36e0f62c4336657b59989454aba0ee7799c35f3bccb4e530"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
